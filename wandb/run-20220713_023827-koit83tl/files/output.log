name: Supervised Classification, features: MiceProtein
{'X_train': {'cols': 0.01443001443001443, 'rows': 0.47050754458161864}, 'X_valid': {'cols': 0.020864381520119227, 'rows': 0.4644808743169399}, 'X_test': {'cols': 0.018518518518518517, 'rows': 0.5222222222222223}}
dataset sizes
(768, 77) (183, 77) (270, 77)
dropped dataset sizes
(406, 77) (98, 77) (129, 77)
key: 8035, k: 1/4, dataset: 40966, missing: None, impute: None
training gbm for 5000 epochs
{'num_leaves': 15, 'max_bin': 47, 'max_depth': 7, 'min_data_in_leaf': 10, 'learning_rate': 0.09994503402661732, 'num_iterations': 109, 'objective': 'softmax', 'verbose': -1, 'num_class': 8}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[106]	valid_0's multi_logloss: 0.0177815
strategy:None, acc gbm: 0.9962962962962963
strategy:None, nll xbg:0.022694990038871765
name: Supervised Classification, features: MiceProtein
{'X_train': {'cols': 0.016590451801719405, 'rows': 0.48450704225352115}, 'X_valid': {'cols': 0.017145775572741866, 'rows': 0.47191011235955055}, 'X_test': {'cols': 0.014718614718614718, 'rows': 0.48148148148148145}}
dataset sizes
(744, 77) (178, 77) (270, 77)
dropped dataset sizes
(379, 77) (94, 77) (140, 77)
key: 1474, k: 2/4, dataset: 40966, missing: None, impute: None
training gbm for 5000 epochs
{'num_leaves': 15, 'max_bin': 47, 'max_depth': 7, 'min_data_in_leaf': 10, 'learning_rate': 0.09994503402661732, 'num_iterations': 109, 'objective': 'softmax', 'verbose': -1, 'num_class': 8}
Training until validation scores don't improve for 100 rounds
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
Did not meet early stopping. Best iteration is:
[75]	valid_0's multi_logloss: 0.0469161
strategy:None, acc gbm: 0.9814814814814815
strategy:None, nll xbg:0.06979834288358688
name: Supervised Classification, features: MiceProtein
{'X_train': {'cols': 0.01694687741199369, 'rows': 0.49612403100775193}, 'X_valid': {'cols': 0.013790333377962243, 'rows': 0.4587628865979381}, 'X_test': {'cols': 0.017075517075517074, 'rows': 0.4703703703703704}}
dataset sizes
(832, 77) (194, 77) (270, 77)
dropped dataset sizes
(417, 77) (105, 77) (143, 77)
key: 9018, k: 3/4, dataset: 40966, missing: None, impute: None
training gbm for 5000 epochs
{'num_leaves': 15, 'max_bin': 47, 'max_depth': 7, 'min_data_in_leaf': 10, 'learning_rate': 0.09994503402661732, 'num_iterations': 109, 'objective': 'softmax', 'verbose': -1, 'num_class': 8}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[109]	valid_0's multi_logloss: 0.0465158
strategy:None, acc gbm: 0.9888888888888889
strategy:None, nll xbg:0.026695678010582924
name: Supervised Classification, features: MiceProtein
{'X_train': {'cols': 0.01616318464144551, 'rows': 0.48097826086956524}, 'X_valid': {'cols': 0.016022021456804068, 'rows': 0.483695652173913}, 'X_test': {'cols': 0.01683501683501683, 'rows': 0.48148148148148145}}
dataset sizes
(768, 77) (184, 77) (270, 77)
dropped dataset sizes
(399, 77) (95, 77) (140, 77)
key: 746, k: 4/4, dataset: 40966, missing: None, impute: None
training gbm for 5000 epochs
{'num_leaves': 15, 'max_bin': 47, 'max_depth': 7, 'min_data_in_leaf': 10, 'learning_rate': 0.09994503402661732, 'num_iterations': 109, 'objective': 'softmax', 'verbose': -1, 'num_class': 8}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[109]	valid_0's multi_logloss: 0.0336794
strategy:None, acc gbm: 0.9629629629629629
strategy:None, nll xbg:0.09278028458356857