name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1393405600722674, 'rows': 0.8048780487804879}, 'X_valid': {'cols': 0.13978494623655915, 'rows': 0.8064516129032258}, 'X_test': {'cols': 0.14166666666666666, 'rows': 0.81}}
dataset sizes
(388, 12) (93, 12) (100, 12)
dropped dataset sizes
(74, 12) (18, 12) (19, 12)
key: 9284, k: 1/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 2, 'decoder_heads': 5, 'decoder_layers': 2, 'net_size': 32, 'net_layers': 2, 'max_steps': 5000, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 5.0}
MAP
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]

 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                  | 8/12 [00:00<00:00, 74.03it/s]
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32


  4%|███████▎                                                                                                                                                                                                   | 15/416 [00:14<01:20,  4.99it/s, l=1.43, tl=inf, tlc=1.48, tc=0, e=-.693]



 17%|██████████████████████████████████▋                                                                                                                                                                        | 71/416 [00:24<01:04,  5.32it/s, l=1.38, tl=inf, tlc=1.36, tc=0, e=-.693]

 25%|██████████████████████████████████████████████████▌                                                                                                                                                       | 104/416 [00:30<00:49,  6.28it/s, l=1.35, tl=inf, tlc=1.39, tc=0, e=-.692]


 39%|███████████████████████████████████████████████████████████████████████████████▌                                                                                                                           | 163/416 [00:40<00:37,  6.66it/s, l=1.33, tl=inf, tlc=1.4, tc=0, e=-.692]

 45%|███████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                               | 187/416 [00:44<00:36,  6.24it/s, l=1.3, tl=inf, tlc=1.44, tc=0, e=-.692]

 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                   | 210/416 [00:48<00:40,  5.12it/s, l=1.33, tl=1.34, tlc=1.34, tc=0, e=-.692]

 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                         | 230/416 [00:52<00:33,  5.57it/s, l=1.32, tl=1.34, tlc=1.43, tc=48, e=-.692]

 63%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                           | 261/416 [00:58<00:26,  5.86it/s, l=1.31, tl=1.29, tlc=1.5, tc=58, e=-.692]

 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                             | 9/12 [00:00<00:00, 87.75it/s]


  7%|████████████████▍                                                                                                                                                                                                                                     | 1/15 [00:01<00:24,  1.74s/it]
(1000, 2)
softmax
1.6769344 -0.5596682
0.7297078 0.27029222

strategy:None, acc lsam:0.5699999928474426
strategy:None, nll lsam:1.3769959211349487
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.690326
strategy:None, acc gbm: 0.55
strategy:None, nll xbg:1.3799974918365479
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1383116883116883, 'rows': 0.7948051948051948}, 'X_valid': {'cols': 0.12714776632302408, 'rows': 0.7731958762886598}, 'X_test': {'cols': 0.14416666666666667, 'rows': 0.81}}
dataset sizes
(394, 12) (97, 12) (100, 12)
dropped dataset sizes
(81, 12) (22, 12) (19, 12)
key: 7746, k: 2/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 2, 'decoder_heads': 5, 'decoder_layers': 2, 'net_size': 32, 'net_layers': 2, 'max_steps': 5000, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 5.0}
MAP
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]


 58%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                      | 7/12 [00:00<00:00, 68.20it/s]
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32


  4%|████████▎                                                                                                                                                                                                  | 17/416 [00:11<01:06,  6.03it/s, l=1.39, tl=inf, tlc=1.39, tc=0, e=-.693]















 77%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                              | 320/416 [01:03<00:16,  5.66it/s, l=1.38, tl=1.39, tlc=1.39, tc=87, e=-.693]



  0%|                                                                                                                                                                                                                                                              | 0/12 [00:00<?, ?it/s]

Final test loss: 1.3900152444839478, epoch: 394, time: 0:01:15.995824
(1000, 2)
softmax
0.020043679 -0.031812597
0.5129612 0.48703888
strategy:None, acc lsam:0.5099999904632568
strategy:None, nll lsam:1.3855195045471191
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.679332
strategy:None, acc gbm: 0.5
strategy:None, nll xbg:1.385874629020691
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.13679245283018868, 'rows': 0.816711590296496}, 'X_valid': {'cols': 0.13620071684587814, 'rows': 0.7634408602150538}, 'X_test': {'cols': 0.14833333333333334, 'rows': 0.81}}
dataset sizes
(380, 12) (93, 12) (100, 12)
dropped dataset sizes
(72, 12) (22, 12) (19, 12)
key: 6142, k: 3/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 2, 'decoder_heads': 5, 'decoder_layers': 2, 'net_size': 32, 'net_layers': 2, 'max_steps': 5000, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 5.0}
MAP
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]


  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)

  3%|█████▊                                                                                                                                                                                                     | 13/454 [00:09<01:20,  5.46it/s, l=1.41, tl=inf, tlc=1.43, tc=0, e=-.693]









 44%|█████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                | 202/454 [00:40<00:37,  6.71it/s, l=1.39, tl=inf, tlc=1.39, tc=0, e=-.693]







 68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                | 309/454 [00:58<00:23,  6.09it/s, l=1.39, tl=1.39, tlc=1.39, tc=77, e=-.693]

 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                            | 9/11 [00:00<00:00, 87.51it/s]

Final test loss: 1.390740156173706, epoch: 330, time: 0:01:00.635695
(1000, 2)
softmax
0.019349724 -0.024058726
0.5108504 0.48914963
strategy:None, acc lsam:0.41999998688697815
strategy:None, nll lsam:1.393710732460022
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.68782
strategy:None, acc gbm: 0.45
strategy:None, nll xbg:1.3904883861541748
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.13553113553113552, 'rows': 0.7664835164835165}, 'X_valid': {'cols': 0.15670289855072464, 'rows': 0.8695652173913043}, 'X_test': {'cols': 0.12833333333333333, 'rows': 0.83}}
dataset sizes
(366, 12) (92, 12) (100, 12)
dropped dataset sizes
(86, 12) (12, 12) (17, 12)
key: 3802, k: 4/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 2, 'decoder_heads': 5, 'decoder_layers': 2, 'net_size': 32, 'net_layers': 2, 'max_steps': 5000, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 5.0}
MAP
2 device(s)


  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)



  File "/home/jahan/Missingness/train.py", line 139, in <module>                                                                                                                                                                                                   | 0/11 [00:00<?, ?it/s]
    metrics_df, perc_missing = run(
  File "/home/jahan/Missingness/benchmarkaux/openmlrun.py", line 155, in run
    model.fit(X_train, y_train)
  File "/home/jahan/Missingness/UAT/models/scikit_wrapper.py", line 152, in fit
    params, history, rng = training_loop(
  File "/home/jahan/Missingness/UAT/training/train.py", line 277, in training_loop
    params, opt_state = take_step(step, params, batch_x, batch_y, key, opt_state, boolean)
  File "<string>", line 1, in <lambda>
KeyboardInterrupt