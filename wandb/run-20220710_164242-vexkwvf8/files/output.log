name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1348238482384824, 'rows': 0.7831978319783198}, 'X_valid': {'cols': 0.15770609318996415, 'rows': 0.8709677419354839}, 'X_test': {'cols': 0.14166666666666666, 'rows': 0.81}}
dataset sizes
(370, 12) (93, 12) (100, 12)
dropped dataset sizes
(80, 12) (12, 12) (19, 12)
key: 412, k: 1/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'max_steps': 10000.0, 'learning_rate': 0.0001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]


  0%|                                                                                                                                                                                                                                                             | 0/909 [00:00<?, ?it/s]
early stop or epoch
2 device(s)
optimizer: adam, lr: 9.999999747378752e-05, batch_size=32




  5%|█████████▌                                                                                                                                                                                                 | 43/909 [00:21<03:02,  4.76it/s, l=1.39, tl=inf, tlc=1.39, tc=0, e=-.693]

  7%|██████████████▎                                                                                                                                                                                            | 64/909 [00:25<02:47,  5.05it/s, l=1.37, tl=inf, tlc=1.38, tc=0, e=-.693]





























 66%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                    | 599/909 [02:20<01:07,  4.57it/s, l=1.25, tl=1.4, tlc=1.57, tc=146, e=-.693]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]


Final test loss: 1.4016152620315552, epoch: 645, time: 0:02:28.273587
(1000, 2)
softmax
0.8730188 -1.0995529
0.82636565 0.17363437
strategy:None, acc lsam:0.5099999904632568
strategy:None, nll lsam:1.439278483390808
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[1]	valid_0's multi_logloss: 0.693167
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
strategy:None, acc gbm: 0.64
strategy:None, nll xbg:1.3860113620758057
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.13722943722943723, 'rows': 0.8}, 'X_valid': {'cols': 0.140893470790378, 'rows': 0.8041237113402062}, 'X_test': {'cols': 0.14416666666666667, 'rows': 0.81}}
dataset sizes
(406, 12) (97, 12) (100, 12)
dropped dataset sizes
(82, 12) (19, 12) (19, 12)
key: 2067, k: 2/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'max_steps': 10000.0, 'learning_rate': 0.0001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/12 [00:00<?, ?it/s]
early stop or epoch
2 device(s)

optimizer: adam, lr: 9.999999747378752e-05, batch_size=32



  3%|█████▌                                                                                                                                                                                                     | 23/833 [00:15<03:16,  4.12it/s, l=1.38, tl=inf, tlc=1.41, tc=0, e=-.693]

  6%|████████████                                                                                                                                                                                                | 49/833 [00:21<03:10,  4.11it/s, l=1.39, tl=inf, tlc=1.4, tc=0, e=-.693]








  File "/home/jahan/Missingness/train.py", line 139, in <module>                                                                                                                                                                                                   | 0/12 [00:00<?, ?it/s]
    metrics_df, perc_missing = run(
  File "/home/jahan/Missingness/benchmarkaux/openmlrun.py", line 155, in run
    model.fit(X_train, y_train)
  File "/home/jahan/Missingness/UAT/models/scikit_wrapper.py", line 152, in fit
    params, history, rng = training_loop(
  File "/home/jahan/Missingness/UAT/training/train.py", line 277, in training_loop
    params, opt_state = take_step(step, params, batch_x, batch_y, key, opt_state, boolean)
  File "<string>", line 1, in <lambda>
KeyboardInterrupt