name: Supervised Regression, features: tecator
{'X_train': {'cols': 0.0, 'rows': 0.0}, 'X_valid': {'cols': 0.0, 'rows': 0.0}, 'X_test': {'cols': 0.0, 'rows': 0.0}}
dataset sizes
(153, 124) (39, 124) (48, 124)
dropped dataset sizes
(153, 124) (39, 124) (48, 124)
key: 3615, k: 1/20, dataset: 505, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 2, 'decoder_heads': 5, 'decoder_layers': 2, 'net_size': 32, 'net_layers': 2, 'max_steps': 5000, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 5.0}
MAP
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32


 35%|████████████████████████████████████████████████████████████████████▏                                                                                                                                | 433/1250 [00:43<00:56, 14.39it/s, l=0.0172, tl=inf, tlc=0.0778, tc=0, e=-.693]

 46%|███████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                         | 581/1250 [00:53<00:44, 14.91it/s, l=0.0161, tl=inf, tlc=0.0361, tc=0, e=-.692]

 54%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                        | 672/1250 [00:59<00:39, 14.53it/s, l=0.00927, tl=0.00717, tlc=0.0243, tc=8, e=-.692]


 69%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                          | 868/1250 [01:13<00:25, 15.07it/s, l=0.00622, tl=0.00551, tlc=0.0155, tc=154, e=-.692]
  0%|                                                                                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]

Final test loss: 0.005506095010787249, epoch: 870, time: 0:01:12.064924
strategy:None, rmse lsam:1.290230393409729
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'regression', 'verbose': -1, 'num_class': 1}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 0.759859
strategy:None, rmse xbg:0.9358275583832564
name: Supervised Regression, features: tecator
{'X_train': {'cols': 0.0, 'rows': 0.0}, 'X_valid': {'cols': 0.0, 'rows': 0.0}, 'X_test': {'cols': 0.0, 'rows': 0.0}}
dataset sizes
(153, 124) (39, 124) (48, 124)
dropped dataset sizes
(153, 124) (39, 124) (48, 124)
key: 6678, k: 2/20, dataset: 505, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 2, 'decoder_heads': 5, 'decoder_layers': 2, 'net_size': 32, 'net_layers': 2, 'max_steps': 5000, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 5.0}
MAP
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
                                                                                                                                                                                                                                                                                          /home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/basic.py:179: UserWarning: Converting column-vector to 1d array
  _log_warning('Converting column-vector to 1d array')
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]
early stop or epoch
2 device(s)

 13%|█████████████████████████▎                                                                                                                                                                           | 161/1250 [00:19<01:26, 12.60it/s, l=0.0557, tl=inf, tlc=0.0898, tc=0, e=-.693]

 26%|███████████████████████████████████████████████████▋                                                                                                                                                 | 328/1250 [00:31<01:01, 14.97it/s, l=0.0253, tl=inf, tlc=0.0624, tc=0, e=-.693]

 31%|█████████████████████████████████████████████████████████████▍                                                                                                                                       | 390/1250 [00:36<00:57, 14.85it/s, l=0.0197, tl=inf, tlc=0.0229, tc=0, e=-.692]

 48%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                       | 594/1250 [00:49<00:44, 14.63it/s, l=0.0107, tl=inf, tlc=0.0202, tc=0, e=-.692]

 55%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                      | 685/1250 [00:55<00:31, 18.00it/s, l=0.00909, tl=0.0144, tlc=0.0286, tc=12, e=-.692]

 71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                       | 887/1250 [01:09<00:23, 15.61it/s, l=0.00607, tl=0.0111, tlc=0.0162, tc=32, e=-.692]

 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                        | 986/1250 [01:16<00:17, 15.51it/s, l=0.00607, tl=0.00967, tlc=0.015, tc=24, e=-.692]
  0%|                                                                                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]

Final test loss: 0.009666194207966328, epoch: 1119, time: 0:01:24.152805
strategy:None, rmse lsam:1.2913684844970703
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'regression', 'verbose': -1, 'num_class': 1}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 0.844434
strategy:None, rmse xbg:0.9378436608154176
name: Supervised Regression, features: tecator
{'X_train': {'cols': 0.0, 'rows': 0.0}, 'X_valid': {'cols': 0.0, 'rows': 0.0}, 'X_test': {'cols': 0.0, 'rows': 0.0}}
dataset sizes
(153, 124) (39, 124) (48, 124)
dropped dataset sizes
(153, 124) (39, 124) (48, 124)
key: 4426, k: 3/20, dataset: 505, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 2, 'decoder_heads': 5, 'decoder_layers': 2, 'net_size': 32, 'net_layers': 2, 'max_steps': 5000, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 5.0}
MAP
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
                                                                                                                                                                                                                                                                                          /home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/basic.py:179: UserWarning: Converting column-vector to 1d array
  _log_warning('Converting column-vector to 1d array')
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32



 29%|████████████████████████████████████████████████████████▍                                                                                                                                            | 358/1250 [00:32<01:02, 14.30it/s, l=0.0264, tl=inf, tlc=0.0229, tc=0, e=-.693]

 43%|████████████████████████████████████████████████████████████████████████████████████                                                                                                                 | 533/1250 [00:44<00:49, 14.49it/s, l=0.0189, tl=inf, tlc=0.0118, tc=0, e=-.692]

 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 714/1250 [00:56<00:35, 15.23it/s, l=0.00788, tl=0.00563, tlc=0.00839, tc=28, e=-.692]
  0%|                                                                                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]
Final test loss: 0.0050437478348612785, epoch: 895, time: 0:01:08.512131
strategy:None, rmse lsam:1.5889763832092285
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'regression', 'verbose': -1, 'num_class': 1}

Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 0.522411
strategy:None, rmse xbg:1.140572039299821
name: Supervised Regression, features: tecator
{'X_train': {'cols': 0.0, 'rows': 0.0}, 'X_valid': {'cols': 0.0, 'rows': 0.0}, 'X_test': {'cols': 0.0, 'rows': 0.0}}
dataset sizes
(153, 124) (39, 124) (48, 124)
dropped dataset sizes
(153, 124) (39, 124) (48, 124)
key: 5458, k: 4/20, dataset: 505, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 2, 'decoder_heads': 5, 'decoder_layers': 2, 'net_size': 32, 'net_layers': 2, 'max_steps': 5000, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 5.0}
MAP
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
                                                                                                                                                                                                                                                                                          /home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/basic.py:179: UserWarning: Converting column-vector to 1d array
  _log_warning('Converting column-vector to 1d array')
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32



 11%|██████████████████████                                                                                                                                                                               | 140/1250 [00:18<01:09, 15.93it/s, l=0.0512, tl=inf, tlc=0.0573, tc=0, e=-.693]

 16%|███████████████████████████████▊                                                                                                                                                                     | 202/1250 [00:22<01:04, 16.22it/s, l=0.0279, tl=inf, tlc=0.0323, tc=0, e=-.693]

 46%|██████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                          | 577/1250 [00:48<00:48, 13.92it/s, l=0.0151, tl=inf, tlc=0.0273, tc=0, e=-.693]

  0%|                                                                                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]
Final test loss: 0.011652126908302307, epoch: 829, time: 0:01:05.321250
strategy:None, rmse lsam:1.496330976486206
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'regression', 'verbose': -1, 'num_class': 1}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 1.0361
strategy:None, rmse xbg:1.0686608094960326
name: Supervised Regression, features: tecator
{'X_train': {'cols': 0.0, 'rows': 0.0}, 'X_valid': {'cols': 0.0, 'rows': 0.0}, 'X_test': {'cols': 0.0, 'rows': 0.0}}
dataset sizes
(153, 124) (39, 124) (48, 124)
dropped dataset sizes
(153, 124) (39, 124) (48, 124)
key: 2755, k: 5/20, dataset: 505, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 2, 'decoder_heads': 5, 'decoder_layers': 2, 'net_size': 32, 'net_layers': 2, 'max_steps': 5000, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 5.0}
MAP
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
                                                                                                                                                                                                                                                                                          /home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/basic.py:179: UserWarning: Converting column-vector to 1d array
  _log_warning('Converting column-vector to 1d array')
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]
early stop or epoch
2 device(s)

 36%|██████████████████████████████████████████████████████████████████████▋                                                                                                                               | 446/1250 [00:37<00:53, 14.91it/s, l=0.026, tl=inf, tlc=0.0381, tc=0, e=-.692]

 42%|███████████████████████████████████████████████████████████████████████████████████▏                                                                                                                 | 528/1250 [00:43<00:55, 13.09it/s, l=0.0252, tl=inf, tlc=0.0141, tc=0, e=-.692]
  0%|                                                                                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]
Final test loss: 0.010859382338821888, epoch: 800, time: 0:01:02.854977
strategy:None, rmse lsam:1.2397483587265015
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'regression', 'verbose': -1, 'num_class': 1}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's l2: 0.706382
strategy:None, rmse xbg:0.9073644191518945
name: Supervised Regression, features: tecator
{'X_train': {'cols': 0.0, 'rows': 0.0}, 'X_valid': {'cols': 0.0, 'rows': 0.0}, 'X_test': {'cols': 0.0, 'rows': 0.0}}
dataset sizes
(153, 124) (39, 124) (48, 124)
dropped dataset sizes
(153, 124) (39, 124) (48, 124)
key: 1304, k: 6/20, dataset: 505, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 2, 'decoder_heads': 5, 'decoder_layers': 2, 'net_size': 32, 'net_layers': 2, 'max_steps': 5000, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 5.0}
MAP
2 device(s)
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/basic.py:179: UserWarning: Converting column-vector to 1d array
  _log_warning('Converting column-vector to 1d array')
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32
  File "/home/jahan/Missingness/train.py", line 139, in <module>                                                                                                                                                                                                    | 0/4 [00:00<?, ?it/s]
    metrics_df, perc_missing = run(
  File "/home/jahan/Missingness/benchmarkaux/openmlrun.py", line 155, in run
    model.fit(X_train, y_train)
  File "/home/jahan/Missingness/UAT/models/scikit_wrapper.py", line 152, in fit
    params, history, rng = training_loop(
  File "/home/jahan/Missingness/UAT/training/train.py", line 277, in training_loop
    params, opt_state = take_step(step, params, batch_x, batch_y, key, opt_state, boolean)
  File "<string>", line 1, in <lambda>
KeyboardInterrupt