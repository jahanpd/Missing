name: Supervised Classification, features: eucalyptus
{'X_train': {'cols': 0.028881165177093673, 'rows': 0.12106918238993711}, 'X_valid': {'cols': 0.023833167825223434, 'rows': 0.0880503144654088}, 'X_test': {'cols': 0.03175057208237986, 'rows': 0.1358695652173913}}
dataset sizes
(670, 19) (159, 19) (184, 19)
dropped dataset sizes
(589, 19) (145, 19) (159, 19)
key: 4879, k: 1/4, dataset: 188, missing: None, impute: None
training gbm for 5000 epochs
{'num_leaves': 20, 'max_bin': 10, 'max_depth': 30, 'min_data_in_leaf': 25, 'learning_rate': 0.022830430215246555, 'num_iterations': 157, 'objective': 'softmax', 'verbose': -1, 'num_class': 5}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[157]	valid_0's multi_logloss: 0.549805
strategy:None, acc gbm: 0.5978260869565217
strategy:None, nll xbg:0.8872780203819275
name: Supervised Classification, features: eucalyptus
{'X_train': {'cols': 0.02133050016345211, 'rows': 0.09782608695652174}, 'X_valid': {'cols': 0.04609349460608042, 'rows': 0.16770186335403728}, 'X_test': {'cols': 0.028318077803203664, 'rows': 0.11413043478260869}}
dataset sizes
(680, 19) (161, 19) (184, 19)
dropped dataset sizes
(608, 19) (134, 19) (163, 19)
key: 8115, k: 2/4, dataset: 188, missing: None, impute: None
training gbm for 5000 epochs
{'num_leaves': 20, 'max_bin': 10, 'max_depth': 30, 'min_data_in_leaf': 25, 'learning_rate': 0.022830430215246555, 'num_iterations': 157, 'objective': 'softmax', 'verbose': -1, 'num_class': 5}
Training until validation scores don't improve for 100 rounds
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
Did not meet early stopping. Best iteration is:
[157]	valid_0's multi_logloss: 0.515747
strategy:None, acc gbm: 0.6032608695652174
strategy:None, nll xbg:0.9321883916854858
name: Supervised Classification, features: eucalyptus
{'X_train': {'cols': 0.028980679546968684, 'rows': 0.11550632911392406}, 'X_valid': {'cols': 0.020319786808794135, 'rows': 0.08860759493670886}, 'X_test': {'cols': 0.02974828375286041, 'rows': 0.11956521739130435}}
dataset sizes
(650, 19) (158, 19) (184, 19)
dropped dataset sizes
(576, 19) (144, 19) (162, 19)
key: 6869, k: 3/4, dataset: 188, missing: None, impute: None
training gbm for 5000 epochs
{'num_leaves': 20, 'max_bin': 10, 'max_depth': 30, 'min_data_in_leaf': 25, 'learning_rate': 0.022830430215246555, 'num_iterations': 157, 'objective': 'softmax', 'verbose': -1, 'num_class': 5}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[157]	valid_0's multi_logloss: 0.580169
strategy:None, acc gbm: 0.6141304347826086
strategy:None, nll xbg:0.9504879117012024
name: Supervised Classification, features: eucalyptus
{'X_train': {'cols': 0.023186777920410784, 'rows': 0.09908536585365854}, 'X_valid': {'cols': 0.022785622593068038, 'rows': 0.08536585365853659}, 'X_test': {'cols': 0.03832951945080091, 'rows': 0.14673913043478262}}
dataset sizes
(670, 19) (164, 19) (184, 19)
dropped dataset sizes
(602, 19) (150, 19) (157, 19)
key: 9749, k: 4/4, dataset: 188, missing: None, impute: None
training gbm for 5000 epochs
{'num_leaves': 20, 'max_bin': 10, 'max_depth': 30, 'min_data_in_leaf': 25, 'learning_rate': 0.022830430215246555, 'num_iterations': 157, 'objective': 'softmax', 'verbose': -1, 'num_class': 5}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[157]	valid_0's multi_logloss: 0.565495
strategy:None, acc gbm: 0.6413043478260869
strategy:None, nll xbg:0.8489822149276733