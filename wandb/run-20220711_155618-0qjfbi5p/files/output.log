/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.13409961685823754, 'rows': 0.8017241379310345}, 'X_valid': {'cols': 0.14015151515151514, 'rows': 0.7954545454545454}, 'X_test': {'cols': 0.14733333333333334, 'rows': 0.8}}
dataset sizes
(356, 12) (88, 12) (125, 12)
dropped dataset sizes
(71, 12) (18, 12) (25, 12)
key: 8649, k: 1/4, dataset: 23381, missing: None, impute: None
training gbm for 5000 epochs
{'num_leaves': 37, 'max_bin': 36, 'max_depth': 21, 'min_data_in_leaf': 14, 'learning_rate': 0.43748485514066815, 'num_iterations': 778, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[1]	valid_0's multi_logloss: 0.714357
strategy:None, acc gbm: 0.488
strategy:None, nll xbg:1.4338574409484863
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.14259259259259258, 'rows': 0.8}, 'X_valid': {'cols': 0.12870370370370368, 'rows': 0.7666666666666667}, 'X_test': {'cols': 0.146, 'rows': 0.832}}
dataset sizes
(372, 12) (90, 12) (125, 12)
dropped dataset sizes
(72, 12) (21, 12) (21, 12)
key: 2480, k: 2/4, dataset: 23381, missing: None, impute: None
training gbm for 5000 epochs
{'num_leaves': 37, 'max_bin': 36, 'max_depth': 21, 'min_data_in_leaf': 14, 'learning_rate': 0.43748485514066815, 'num_iterations': 778, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[10]	valid_0's multi_logloss: 0.641718
strategy:None, acc gbm: 0.504
strategy:None, nll xbg:1.8881456851959229
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.14583333333333334, 'rows': 0.811046511627907}, 'X_valid': {'cols': 0.13372093023255816, 'rows': 0.7325581395348837}, 'X_test': {'cols': 0.12533333333333332, 'rows': 0.792}}
dataset sizes
(360, 12) (86, 12) (125, 12)
dropped dataset sizes
(69, 12) (23, 12) (26, 12)
key: 4329, k: 3/4, dataset: 23381, missing: None, impute: None
training gbm for 5000 epochs
{'num_leaves': 37, 'max_bin': 36, 'max_depth': 21, 'min_data_in_leaf': 14, 'learning_rate': 0.43748485514066815, 'num_iterations': 778, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[2]	valid_0's multi_logloss: 0.646503
strategy:None, acc gbm: 0.608
strategy:None, nll xbg:1.357470989227295
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.13839724680432644, 'rows': 0.7905604719764012}, 'X_valid': {'cols': 0.13627450980392156, 'rows': 0.8235294117647058}, 'X_test': {'cols': 0.13799999999999998, 'rows': 0.784}}
dataset sizes
(352, 12) (85, 12) (125, 12)
dropped dataset sizes
(75, 12) (15, 12) (27, 12)
key: 4883, k: 4/4, dataset: 23381, missing: None, impute: None
training gbm for 5000 epochs
{'num_leaves': 37, 'max_bin': 36, 'max_depth': 21, 'min_data_in_leaf': 14, 'learning_rate': 0.43748485514066815, 'num_iterations': 778, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Early stopping, best iteration is:
[5]	valid_0's multi_logloss: 0.625878
strategy:None, acc gbm: 0.544
strategy:None, nll xbg:1.4953728914260864