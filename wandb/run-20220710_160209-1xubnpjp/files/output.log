name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1415989159891599, 'rows': 0.7940379403794038}, 'X_valid': {'cols': 0.13172043010752688, 'rows': 0.8064516129032258}, 'X_test': {'cols': 0.14166666666666666, 'rows': 0.81}}
dataset sizes
(370, 12) (93, 12) (100, 12)
dropped dataset sizes
(76, 12) (18, 12) (19, 12)
key: 1976, k: 1/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch

2 device(s)
optimizer: adam, lr: 0.0010000000474974513, batch_size=32


 14%|██████████████████████▍                                                                                                                                         | 14/100 [00:16<00:20,  4.25it/s, l=1.4, tl=inf, tlc=1.39, tc=0, mse=1.93, cea=11.3, e=0.819, pmax=0.504, pmin=0.497]




 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                               | 6/11 [00:00<00:00, 57.90it/s]

Final test loss: 1.386178970336914, epoch: 66, time: 0:00:25.707524
(1000, 2)

  7%|████████████████▍                                                                                                                                                                                                                                     | 1/15 [00:02<00:29,  2.13s/it]
0.028989512 0.027952887
0.5002579 0.4997421
strategy:None, acc lsam:0.4099999964237213
strategy:None, nll lsam:1.3864803314208984
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[89]	valid_0's multi_logloss: 0.691415
strategy:None, acc gbm: 0.62
strategy:None, nll xbg:1.366987943649292
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1383116883116883, 'rows': 0.7922077922077922}, 'X_valid': {'cols': 0.14604810996563572, 'rows': 0.8247422680412371}, 'X_test': {'cols': 0.14416666666666667, 'rows': 0.81}}
dataset sizes
(386, 12) (97, 12) (100, 12)
dropped dataset sizes
(81, 12) (17, 12) (19, 12)
key: 6509, k: 2/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/12 [00:00<?, ?it/s]
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32






  0%|                                                                                                                                                                                                                                                              | 0/12 [00:00<?, ?it/s]


  7%|████████████████▍                                                                                                                                                                                                                                     | 1/15 [00:01<00:18,  1.36s/it]
(1000, 2)
softmax
0.017973196 0.008132015
0.5024239 0.49757606
strategy:None, acc lsam:0.4899999797344208
strategy:None, nll lsam:1.3865108489990234
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.682267
strategy:None, acc gbm: 0.53
strategy:None, nll xbg:1.3800634145736694
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1412848158131177, 'rows': 0.8086253369272237}, 'X_valid': {'cols': 0.1370967741935484, 'rows': 0.7741935483870968}, 'X_test': {'cols': 0.14833333333333334, 'rows': 0.81}}
dataset sizes
(374, 12) (93, 12) (100, 12)
dropped dataset sizes
(72, 12) (21, 12) (19, 12)
key: 1693, k: 3/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]

  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]








  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]


  7%|████████████████▍                                                                                                                                                                                                                                     | 1/15 [00:01<00:16,  1.14s/it]
(1000, 2)
softmax
0.0336561 0.029913316
0.50093436 0.49906564
strategy:None, acc lsam:0.41999998688697815
strategy:None, nll lsam:1.386895775794983
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.686846
strategy:None, acc gbm: 0.5
strategy:None, nll xbg:1.3883779048919678
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.14606227106227107, 'rows': 0.7912087912087912}, 'X_valid': {'cols': 0.14402173913043478, 'rows': 0.8152173913043478}, 'X_test': {'cols': 0.12833333333333333, 'rows': 0.83}}
dataset sizes
(364, 12) (92, 12) (100, 12)
dropped dataset sizes
(76, 12) (17, 12) (17, 12)
key: 1371, k: 4/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)









 45%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                      | 5/11 [00:00<00:00, 48.61it/s]

  7%|████████████████▍                                                                                                                                                                                                                                     | 1/15 [00:01<00:15,  1.11s/it]
(1000, 2)
softmax
0.07813045 0.07209126
0.5015058 0.49849427
strategy:None, acc lsam:0.6200000047683716
strategy:None, nll lsam:1.3848578929901123
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.689771
strategy:None, acc gbm: 0.51
strategy:None, nll xbg:1.3840278387069702
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.14560439560439561, 'rows': 0.8214285714285714}, 'X_valid': {'cols': 0.13043478260869565, 'rows': 0.7717391304347826}, 'X_test': {'cols': 0.13333333333333333, 'rows': 0.75}}
dataset sizes
(372, 12) (92, 12) (100, 12)
dropped dataset sizes
(66, 12) (21, 12) (25, 12)
key: 7016, k: 5/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)






 64%|█████████████████████████████████████████████████████████████████████████████████████████████████████                                                         | 64/100 [00:23<00:07,  4.67it/s, l=1.38, tl=1.39, tlc=1.39, tc=9, mse=2.13, cea=8.48, e=0.814, pmax=0.503, pmin=0.497]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]


  7%|████████████████▍                                                                                                                                                                                                                                     | 1/15 [00:01<00:20,  1.50s/it]
(1000, 2)
softmax
0.14423783 0.08739296
0.51419955 0.4858005
strategy:None, acc lsam:0.6200000047683716
strategy:None, nll lsam:1.3734666109085083
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.682927
strategy:None, acc gbm: 0.49
strategy:None, nll xbg:1.3879042863845825
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.14385727190605238, 'rows': 0.8021680216802168}, 'X_valid': {'cols': 0.12275985663082438, 'rows': 0.7741935483870968}, 'X_test': {'cols': 0.14166666666666666, 'rows': 0.81}}
dataset sizes
(374, 12) (93, 12) (100, 12)
dropped dataset sizes
(73, 12) (21, 12) (19, 12)
key: 3809, k: 6/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)






  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]

/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/12 [00:00<?, ?it/s]
(1000, 2)
softmax
-0.21673065 -0.28530073
0.51713556 0.48286444
strategy:None, acc lsam:0.4099999964237213
strategy:None, nll lsam:1.3998119831085205
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.686877
strategy:None, acc gbm: 0.55
strategy:None, nll xbg:1.37565016746521
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1422077922077922, 'rows': 0.8077922077922078}, 'X_valid': {'cols': 0.13058419243986252, 'rows': 0.7628865979381443}, 'X_test': {'cols': 0.14416666666666667, 'rows': 0.81}}
dataset sizes
(386, 12) (97, 12) (100, 12)
dropped dataset sizes
(74, 12) (23, 12) (19, 12)
key: 5241, k: 7/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)

optimizer: adabelief, lr: 0.001, batch_size=32
early stop or epoch
2 device(s)
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]


 17%|███████████████████████████▏                                                                                                                                    | 17/100 [00:13<00:20,  4.04it/s, l=1.4, tl=inf, tlc=1.39, tc=0, mse=1.91, cea=13.8, e=0.826, pmax=0.504, pmin=0.495]






  0%|                                                                                                                                                                                                                                                              | 0/12 [00:00<?, ?it/s]
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/12 [00:00<?, ?it/s]
(1000, 2)
softmax
-0.41692957 -0.42001238
0.50076824 0.4992318
strategy:None, acc lsam:0.4899999797344208
strategy:None, nll lsam:1.3863581418991089
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.687408
strategy:None, acc gbm: 0.56
strategy:None, nll xbg:1.372736930847168
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1376909254267745, 'rows': 0.784366576819407}, 'X_valid': {'cols': 0.1514336917562724, 'rows': 0.8709677419354839}, 'X_test': {'cols': 0.14833333333333334, 'rows': 0.81}}
dataset sizes
(388, 12) (93, 12) (100, 12)
dropped dataset sizes
(81, 12) (12, 12) (19, 12)
key: 5101, k: 8/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)

optimizer: adabelief, lr: 0.001, batch_size=32
early stop or epoch
2 device(s)
optimizer: adam, lr: 0.0010000000474974513, batch_size=32



 20%|███████████████████████████████▊                                                                                                                               | 20/100 [00:14<00:18,  4.44it/s, l=1.39, tl=inf, tlc=1.39, tc=0, mse=2.25, cea=6.77, e=0.834, pmax=0.504, pmin=0.496]





 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 73/100 [00:26<00:06,  3.87it/s, l=1.39, tl=1.39, tlc=1.39, tc=4, mse=2.22, cea=6.55, e=0.834, pmax=0.504, pmin=0.497]
  0%|                                                                                                                                                                                                                                                              | 0/12 [00:00<?, ?it/s]

Final test loss: 1.3874685764312744, epoch: 82, time: 0:00:28.033548
(1000, 2)
softmax
0.033490676 0.015611693
0.50446695 0.49553296
strategy:None, acc lsam:0.41999998688697815
strategy:None, nll lsam:1.389233112335205
training gbm for 5000 epochs
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.685723
strategy:None, acc gbm: 0.51
strategy:None, nll xbg:1.3898382186889648
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1423992673992674, 'rows': 0.7912087912087912}, 'X_valid': {'cols': 0.1585144927536232, 'rows': 0.8152173913043478}, 'X_test': {'cols': 0.12833333333333333, 'rows': 0.83}}
dataset sizes
(366, 12) (92, 12) (100, 12)
dropped dataset sizes
(76, 12) (17, 12) (17, 12)
key: 5509, k: 9/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32





  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]


  7%|████████████████▍                                                                                                                                                                                                                                     | 1/15 [00:01<00:17,  1.26s/it]
(1000, 2)
softmax
0.30986318 0.29332754
0.5041321 0.4958679
strategy:None, acc lsam:0.3799999952316284
strategy:None, nll lsam:1.3903286457061768
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.688566
strategy:None, acc gbm: 0.55
strategy:None, nll xbg:1.3805203437805176
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1472069597069597, 'rows': 0.8241758241758241}, 'X_valid': {'cols': 0.12409420289855072, 'rows': 0.7608695652173914}, 'X_test': {'cols': 0.13333333333333333, 'rows': 0.75}}
dataset sizes
(374, 12) (92, 12) (100, 12)
dropped dataset sizes
(67, 12) (22, 12) (25, 12)
key: 8440, k: 10/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32








 45%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                      | 5/11 [00:00<00:00, 46.44it/s]


 73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                 | 11/15 [00:03<00:00,  4.27it/s]
(1000, 2)
softmax
0.015852682 -0.021378517
0.5093056 0.4906945
strategy:None, acc lsam:0.6200000047683716
strategy:None, nll lsam:1.377706527709961
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.692329
strategy:None, acc gbm: 0.54
strategy:None, nll xbg:1.3813625574111938
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.13911472448057816, 'rows': 0.8130081300813008}, 'X_valid': {'cols': 0.14157706093189965, 'rows': 0.7311827956989247}, 'X_test': {'cols': 0.14166666666666666, 'rows': 0.81}}
dataset sizes
(378, 12) (93, 12) (100, 12)
dropped dataset sizes
(72, 12) (25, 12) (19, 12)
key: 8397, k: 11/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]

  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)
optimizer: adam, lr: 0.0010000000474974513, batch_size=32









 45%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                      | 5/11 [00:00<00:00, 47.98it/s]

  7%|████████████████▍                                                                                                                                                                                                                                     | 1/15 [00:01<00:15,  1.10s/it]
(1000, 2)
softmax
0.083200105 0.023915922
0.514815 0.48518503
strategy:None, acc lsam:0.5899999737739563
strategy:None, nll lsam:1.3765027523040771
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.683858
strategy:None, acc gbm: 0.58
strategy:None, nll xbg:1.3745570182800293
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.14285714285714285, 'rows': 0.8103896103896104}, 'X_valid': {'cols': 0.1280068728522337, 'rows': 0.7525773195876289}, 'X_test': {'cols': 0.14416666666666667, 'rows': 0.81}}
dataset sizes
(404, 12) (97, 12) (100, 12)
dropped dataset sizes
(73, 12) (24, 12) (19, 12)
key: 3543, k: 12/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]

 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                           | 6/12 [00:00<00:00, 55.77it/s]
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32




 31%|█████████████████████████████████████████████████▎                                                                                                             | 31/100 [00:17<00:16,  4.08it/s, l=1.38, tl=inf, tlc=1.41, tc=0, mse=1.73, cea=7.05, e=0.829, pmax=0.502, pmin=0.498]





  0%|                                                                                                                                                                                                                                                              | 0/12 [00:00<?, ?it/s]

Final test loss: 1.4047995805740356, epoch: 81, time: 0:00:28.291241
(1000, 2)
softmax
0.09118946 -0.0088627245
0.52499163 0.4750084
strategy:None, acc lsam:0.5099999904632568
strategy:None, nll lsam:1.3867946863174438
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[99]	valid_0's multi_logloss: 0.689956
strategy:None, acc gbm: 0.54
strategy:None, nll xbg:1.3797789812088013
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.14330637915543576, 'rows': 0.8032345013477089}, 'X_valid': {'cols': 0.12903225806451613, 'rows': 0.7956989247311828}, 'X_test': {'cols': 0.14833333333333334, 'rows': 0.81}}
dataset sizes
(374, 12) (93, 12) (100, 12)
dropped dataset sizes
(74, 12) (19, 12) (19, 12)
key: 2743, k: 13/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]

  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)
optimizer: adam, lr: 0.0010000000474974513, batch_size=32




 45%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                      | 5/11 [00:00<00:00, 49.69it/s]

Final test loss: 1.3858250379562378, epoch: 66, time: 0:00:22.990274
(1000, 2)
softmax
0.048929043 0.04077607
0.50202966 0.4979704
strategy:None, acc lsam:0.41999998688697815
strategy:None, nll lsam:1.387606143951416
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.686904
strategy:None, acc gbm: 0.51
strategy:None, nll xbg:1.3907201290130615
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.14491758241758243, 'rows': 0.7994505494505495}, 'X_valid': {'cols': 0.14855072463768115, 'rows': 0.782608695652174}, 'X_test': {'cols': 0.12833333333333333, 'rows': 0.83}}
dataset sizes
(366, 12) (92, 12) (100, 12)
dropped dataset sizes
(73, 12) (20, 12) (17, 12)
key: 3568, k: 14/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)







  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]

Final test loss: 1.3860465288162231, epoch: 63, time: 0:00:23.631784
(1000, 2)
softmax
-0.0047429665 -0.047755186
0.51075083 0.48924917
strategy:None, acc lsam:0.6200000047683716
strategy:None, nll lsam:1.3764344453811646
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.688006
strategy:None, acc gbm: 0.62
strategy:None, nll xbg:1.3711811304092407
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1478937728937729, 'rows': 0.8379120879120879}, 'X_valid': {'cols': 0.12137681159420288, 'rows': 0.7065217391304348}, 'X_test': {'cols': 0.13333333333333333, 'rows': 0.75}}
dataset sizes
(370, 12) (92, 12) (100, 12)
dropped dataset sizes
(60, 12) (27, 12) (25, 12)
key: 5701, k: 15/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32







 64%|████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 64/100 [00:24<00:07,  4.57it/s, l=1.38, tl=1.39, tlc=1.39, tc=19, mse=1.78, cea=17.5, e=0.821, pmax=0.503, pmin=0.496]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]

Final test loss: 1.3882687091827393, epoch: 66, time: 0:00:23.662245
(1000, 2)
softmax
-0.0023287728 -0.034819234
0.50812143 0.4918786
strategy:None, acc lsam:0.3799999952316284
strategy:None, nll lsam:1.3943554162979126
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.690131
strategy:None, acc gbm: 0.64
strategy:None, nll xbg:1.365092396736145
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.13730803974706413, 'rows': 0.8021680216802168}, 'X_valid': {'cols': 0.14874551971326164, 'rows': 0.7741935483870968}, 'X_test': {'cols': 0.14166666666666666, 'rows': 0.81}}
dataset sizes
(376, 12) (93, 12) (100, 12)
dropped dataset sizes
(75, 12) (21, 12) (19, 12)
key: 7899, k: 16/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)



 74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                         | 74/100 [00:26<00:06,  4.16it/s, l=1.38, tl=1.39, tlc=1.39, tc=16, mse=2.02, cea=21.8, e=0.812, pmax=0.5, pmin=0.496]
 45%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                      | 5/11 [00:00<00:00, 46.24it/s]

Final test loss: 1.3895516395568848, epoch: 79, time: 0:00:25.752435
(1000, 2)
softmax
0.0093403775 -0.034316607
0.5109123 0.48908773
strategy:None, acc lsam:0.5899999737739563
strategy:None, nll lsam:1.3789129257202148
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.69211
strategy:None, acc gbm: 0.57
strategy:None, nll xbg:1.371757984161377
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1422077922077922, 'rows': 0.7948051948051948}, 'X_valid': {'cols': 0.13058419243986255, 'rows': 0.8144329896907216}, 'X_test': {'cols': 0.14416666666666667, 'rows': 0.81}}
dataset sizes
(386, 12) (97, 12) (100, 12)
dropped dataset sizes
(79, 12) (18, 12) (19, 12)
key: 2554, k: 17/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]

  8%|████████████████████▌                                                                                                                                                                                                                                 | 1/12 [00:04<00:48,  4.42s/it]
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32









 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                        | 10/12 [00:00<00:00, 49.00it/s]
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
(1000, 2)
softmax
0.05816552 0.0477903
0.50259036 0.49740964
strategy:None, acc lsam:0.5099999904632568
strategy:None, nll lsam:1.3861138820648193
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.690938
strategy:None, acc gbm: 0.5
strategy:None, nll xbg:1.3859014511108398
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.1408355795148248, 'rows': 0.8032345013477089}, 'X_valid': {'cols': 0.1388888888888889, 'rows': 0.7956989247311828}, 'X_test': {'cols': 0.14833333333333334, 'rows': 0.81}}
dataset sizes
(374, 12) (93, 12) (100, 12)
dropped dataset sizes
(74, 12) (19, 12) (19, 12)
key: 6833, k: 18/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)

optimizer: adabelief, lr: 0.001, batch_size=32
early stop or epoch
2 device(s)
optimizer: adam, lr: 0.0010000000474974513, batch_size=32








 45%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                      | 5/11 [00:00<00:00, 48.81it/s]

Final test loss: 1.3856241703033447, epoch: 62, time: 0:00:22.803023
(1000, 2)
softmax
-0.030423624 -0.040182266
0.50243455 0.49756545
strategy:None, acc lsam:0.41999998688697815
strategy:None, nll lsam:1.3878750801086426
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.689244
strategy:None, acc gbm: 0.55
strategy:None, nll xbg:1.3783999681472778
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.14560439560439561, 'rows': 0.7967032967032966}, 'X_valid': {'cols': 0.14583333333333334, 'rows': 0.7934782608695652}, 'X_test': {'cols': 0.12833333333333333, 'rows': 0.83}}
dataset sizes
(372, 12) (92, 12) (100, 12)
dropped dataset sizes
(75, 12) (19, 12) (17, 12)
key: 4219, k: 19/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32







 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 10/11 [00:00<00:00, 46.89it/s]

Final test loss: 1.3851882219314575, epoch: 75, time: 0:00:24.421062
(1000, 2)
softmax
-0.055908922 -0.059236296
0.50082767 0.49917233
strategy:None, acc lsam:0.3799999952316284
strategy:None, nll lsam:1.3870906829833984
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[1]	valid_0's multi_logloss: 0.693166
strategy:None, acc gbm: 0.67
strategy:None, nll xbg:1.3860232830047607
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.14033882783882787, 'rows': 0.8076923076923077}, 'X_valid': {'cols': 0.15126811594202896, 'rows': 0.8260869565217391}, 'X_test': {'cols': 0.13333333333333333, 'rows': 0.75}}
dataset sizes
(372, 12) (92, 12) (100, 12)
dropped dataset sizes
(72, 12) (16, 12) (25, 12)
key: 2109, k: 20/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'epochs': 100, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)






 55%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                               | 6/11 [00:00<00:00, 47.49it/s]

/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
(1000, 2)
softmax
-0.015004274 -0.07146738
0.5141114 0.48588857
strategy:None, acc lsam:0.3799999952316284
strategy:None, nll lsam:1.4006417989730835
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.686412
strategy:None, acc gbm: 0.48
strategy:None, nll xbg:1.3906453847885132