name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.14092140921409213, 'rows': 0.8075880758807588}, 'X_valid': {'cols': 0.14157706093189965, 'rows': 0.8064516129032258}, 'X_test': {'cols': 0.14166666666666666, 'rows': 0.81}}
dataset sizes
(370, 12) (93, 12) (100, 12)
dropped dataset sizes
(71, 12) (18, 12) (19, 12)
key: 3414, k: 1/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'max_steps': 10000.0, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]

  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]
early stop or epoch
2 device(s)

  0%|                                                                                                                                                                                                                                                             | 0/909 [00:00<?, ?it/s]










 12%|████████████████████████                                                                                                                                                                                  | 108/909 [00:35<02:44,  4.88it/s, l=1.39, tl=inf, tlc=1.39, tc=0, e=-.693]



 18%|████████████████████████████████████▍                                                                                                                                                                     | 164/909 [00:47<02:32,  4.88it/s, l=1.38, tl=inf, tlc=1.39, tc=0, e=-.693]




 24%|████████████████████████████████████████████████▏                                                                                                                                                         | 217/909 [00:57<02:26,  4.72it/s, l=1.39, tl=inf, tlc=1.39, tc=0, e=-.693]



 30%|████████████████████████████████████████████████████████████▉                                                                                                                                             | 274/909 [01:09<02:15,  4.68it/s, l=1.38, tl=inf, tlc=1.39, tc=0, e=-.693]



 38%|████████████████████████████████████████████████████████████████████████████▋                                                                                                                             | 345/909 [01:23<01:50,  5.09it/s, l=1.39, tl=inf, tlc=1.39, tc=0, e=-.693]



 43%|███████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                  | 394/909 [01:33<01:48,  4.73it/s, l=1.39, tl=inf, tlc=1.39, tc=0, e=-.693]

















 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                     | 665/909 [02:31<00:53,  4.60it/s, l=1.38, tl=1.39, tlc=1.39, tc=161, e=-.693]




  0%|                                                                                                                                                                                                                                                              | 0/11 [00:00<?, ?it/s]


  7%|████████████████▍                                                                                                                                                                                                                                     | 1/15 [00:01<00:26,  1.91s/it]
(1000, 2)
softmax
0.020493649 -0.0131689515

0.50841486 0.49158514
strategy:None, acc lsam:0.4099999964237213
strategy:None, nll lsam:1.3926368951797485
training gbm for 5000 epochs
{'num_leaves': 31, 'max_bin': 155, 'max_depth': -1, 'min_data_in_leaf': 20, 'learning_rate': 0.001, 'num_iterations': 100, 'objective': 'softmax', 'verbose': -1, 'num_class': 2}
Training until validation scores don't improve for 100 rounds
Did not meet early stopping. Best iteration is:
[100]	valid_0's multi_logloss: 0.688425
strategy:None, acc gbm: 0.57
strategy:None, nll xbg:1.377995252609253
name: Supervised Classification, features: dresses-sales
{'X_train': {'cols': 0.14264069264069265, 'rows': 0.8051948051948052}, 'X_valid': {'cols': 0.13659793814432988, 'rows': 0.7835051546391752}, 'X_test': {'cols': 0.14416666666666667, 'rows': 0.81}}
dataset sizes
(400, 12) (97, 12) (100, 12)
dropped dataset sizes
(80, 12) (21, 12) (19, 12)
key: 1500, k: 2/20, dataset: 23381, missing: None, impute: None
{'d_model': 32, 'embedding_size': 32, 'embedding_layers': 2, 'encoder_heads': 5, 'encoder_layers': 5, 'decoder_heads': 5, 'decoder_layers': 5, 'net_size': 32, 'net_layers': 2, 'max_steps': 10000.0, 'learning_rate': 0.001, 'early_stop': 0.5, 'noise_std': 0.1}
MAP
2 device(s)
optimizer: adabelief, lr: 0.001, batch_size=32
/home/jahan/.conda/envs/jax/lib/python3.10/site-packages/lightgbm/engine.py:177: UserWarning: Found `num_iterations` in params. Will use it instead of argument
  _log_warning(f"Found `{alias}` in params. Will use it instead of argument")
  0%|                                                                                                                                                                                                                                                             | 0/100 [00:00<?, ?it/s]

  8%|████████████████████▌                                                                                                                                                                                                                                 | 1/12 [00:04<00:51,  4.68s/it]
early stop or epoch
2 device(s)

optimizer: adam, lr: 0.0010000000474974513, batch_size=32












  File "/home/jahan/Missingness/train.py", line 139, in <module>████████████████████████████████████████████████████████████████████████████████████▌                                                                                                      | 7/12 [00:00<00:00, 65.37it/s]
    metrics_df, perc_missing = run(
  File "/home/jahan/Missingness/benchmarkaux/openmlrun.py", line 155, in run
    model.fit(X_train, y_train)
  File "/home/jahan/Missingness/UAT/models/scikit_wrapper.py", line 152, in fit
    params, history, rng = training_loop(
  File "/home/jahan/Missingness/UAT/training/train.py", line 277, in training_loop
    params, opt_state = take_step(step, params, batch_x, batch_y, key, opt_state, boolean)
  File "<string>", line 1, in <lambda>
KeyboardInterrupt